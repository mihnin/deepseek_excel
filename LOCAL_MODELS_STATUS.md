# –°—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

## ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞

### 1. –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:

#### Ollama
- **–ü–æ—Ä—Ç**: 11434 ‚úÖ
- **Base URL**: `http://localhost:11434` ‚úÖ
- **API endpoints**: 
  - `/api/tags` - —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ‚úÖ
  - `/api/chat` - —á–∞—Ç endpoint ‚úÖ
  - `/v1/chat/completions` - OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (–Ω–æ–≤–æ–µ –≤ 2024) ‚úÖ

#### LM Studio  
- **–ü–æ—Ä—Ç**: 1234 ‚úÖ
- **Base URL**: `http://localhost:1234/v1` ‚úÖ
- **OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –ü–æ–ª–Ω–∞—è ‚úÖ

#### Text Generation WebUI (Oobabooga)
- **–ü–æ—Ä—Ç API**: 5000 ‚úÖ
- **Base URL**: `http://localhost:5000/v1` ‚úÖ
- **OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –ü–æ–ª–Ω–∞—è ‚úÖ

### 2. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –ø—É—Ç–∏ API –≤ –∫–æ–¥–µ:
- –£–±—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ `/v1` –¥–ª—è LM Studio –∏ Text Gen WebUI
- –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API –¥–ª—è Ollama

### 3. –°–æ–∑–¥–∞–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã:
- `docs/LOCAL_MODELS_SETUP.md` - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ
- `docs/LMSTUDIO_SETUP.md` - —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞ –¥–ª—è LM Studio
- `scripts/test_local_models.py` - —Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

## üìã –ö–æ–¥ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

### –§–∞–π–ª: `src/llm/local_provider.py`

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏** (–º–µ—Ç–æ–¥ `_check_availability`):
```python
# Ollama
if self.provider == "ollama":
    response = requests.get(f"{self.base_url}/api/tags", timeout=5)
    
# LM Studio (URL —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç /v1)
elif self.provider == "lmstudio":
    response = requests.get(f"{self.base_url}/models", timeout=5)
    
# Text Gen WebUI (URL —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç /v1)
elif self.provider == "textgen_webui":
    response = requests.get(f"{self.base_url}/models", timeout=5)
```

**–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤**:
- Ollama –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ `_ollama_completion` —Å endpoint `/api/chat`
- LM Studio –∏ Text Gen WebUI –∏—Å–ø–æ–ª—å–∑—É—é—Ç `_openai_compatible_completion` —Å endpoint `/chat/completions`

### –§–∞–π–ª: `config/default_config.json`

```json
"base_urls": {
    "ollama": "http://localhost:11434",
    "lmstudio": "http://localhost:1234/v1",
    "textgen_webui": "http://localhost:5000/v1",
    "custom": "http://localhost:8000"
}
```

## üöÄ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

### 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä:

#### Ollama
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
# Windows: —Å–∫–∞—á–∞–π—Ç–µ —Å https://ollama.com

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
ollama serve

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
ollama pull llama3
```

#### LM Studio
1. –û—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ LM Studio
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ "Discover"
3. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ "Local Server"
4. –ù–∞–∂–º–∏—Ç–µ "Start Server"

#### Text Generation WebUI
```bash
cd text-generation-webui
python server.py --api --api-port 5000
```

### 2. –í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ DeepSeek Excel:
1. –û—Ç–∫—Ä–æ–π—Ç–µ "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM"
2. –í—ã–±–µ—Ä–∏—Ç–µ "–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å"
3. –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
4. –ù–∞–∂–º–∏—Ç–µ "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
python scripts/test_local_models.py
```

## ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

–ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 2024 –≥–æ–¥–∞ –¥–ª—è:
- ‚úÖ **Ollama** - –≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–π OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
- ‚úÖ **LM Studio** - –ø–æ–ª–Ω–∞—è OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å  
- ‚úÖ **Text Generation WebUI** - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π API –Ω–∞ –ø–æ—Ä—Ç—É 5000

–í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ –∏—Ö –∑–∞–ø—É—Å–∫–∞ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ—Ä—Ç–∞—Ö.